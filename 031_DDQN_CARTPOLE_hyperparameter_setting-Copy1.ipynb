{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DDQN with Hyperparameter setting**\n",
    "\n",
    "This is an example of Hyperparameter setting for CARTPOLE. <br>\n",
    "With the actual setup it can take many hours (even with a GPU) <br>\n",
    "Press the colab icon to run it in COLAB (in this way you save your laptop for other tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFJSVpVdhkJ6",
    "outputId": "1d20e9ec-d059-44ea-ddad-df428c96ba22"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/castorgit/RL_course/blob/main/00_LunarLander-COLAB_render.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 09:53:46.964305: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-21 09:53:46.976206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-21 09:53:46.990538: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-21 09:53:46.994798: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-21 09:53:47.005543: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-21 09:53:48.005566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.0\n",
      "2.17.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as k\n",
    "print(k.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "ERROR: unknown command \"pandas\"\n"
     ]
    }
   ],
   "source": [
    "#### **Install Packages**\n",
    "!pip install gymnasium[box2d]\n",
    "!pip install numpy\n",
    "!pip install random\n",
    "!pip install matplotlib\n",
    "!pip install tensorflow==2.17.1\n",
    "!pip install keras==3.6.0\n",
    "!pip pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XmduteTzhmVH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rxswekI1hmXw"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]  # 8 state variables\n",
    "action_size = env.action_space.n  # 4 discrete actions\n",
    "tf.random.set_seed(221)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **hyperparameter search space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.0001],\n",
    "    'gamma': [0.99],\n",
    "    'batch_size': [128],\n",
    "    'tau': [0.1],\n",
    "    'epsilon_decay': [0.99],\n",
    "    'retrain_steps': [15, 10] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zonyAYX-hmad"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "gamma = 0.98\n",
    "batch_size = 128\n",
    "epsilon_start = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.99\n",
    "tau = 0.15  # For soft target network update\n",
    "buffer_capacity = 10000\n",
    "max_episodes = 100                  # We train only 50 to see how the agent learns\n",
    "max_steps = 1200\n",
    "solved_threshold = 195\n",
    "verbose = 0                        # 0: No trace 1: Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IVrsk55q1qJY"
   },
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "replay_buffer = deque(maxlen=buffer_capacity)\n",
    "\n",
    "# Add experience to replay buffer\n",
    "def store_experience(state, action, reward, next_state, done):\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "# Sample experiences from the replay buffer\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.choice(len(replay_buffer), batch_size, replace=False)\n",
    "    batch = [replay_buffer[i] for i in indices]\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    return (\n",
    "        np.vstack(states),\n",
    "        np.array(actions),\n",
    "        np.array(rewards),\n",
    "        np.vstack(next_states),\n",
    "        np.array(dones, dtype=np.float32)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Neural Network definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network\n",
    "def build_model(state_size, action_size):\n",
    "    inputs = Input(shape=(state_size,))  \n",
    "    x = Dense(16, activation=\"relu\")(inputs)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    outputs = Dense(action_size, activation=\"linear\")(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Support Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bcqSPBvyw3OQ"
   },
   "outputs": [],
   "source": [
    "# Soft update function for the target network\n",
    "def soft_update(model, target_model, tau):\n",
    "    target_weights = target_model.get_weights()\n",
    "    model_weights = model.get_weights()\n",
    "    new_weights = [\n",
    "        tau * mw + (1 - tau) * tw for mw, tw in zip(model_weights, target_weights)\n",
    "    ]\n",
    "    target_model.set_weights(new_weights)\n",
    "\n",
    "# Polski Optimization Function\n",
    "def polski_optimization(weights, beta=0.01):\n",
    "    return [w * (1 - beta) for w in weights]\n",
    "\n",
    "# Double DQN target calculation\n",
    "def experience_replay_with_ddqn(model, target_model, batch_size, gamma, tau, step):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "\n",
    "    # Predict Q-values for next states using both networks\n",
    "    next_q_values = model.predict(next_states, verbose=0)\n",
    "    best_actions = np.argmax(next_q_values, axis=1)\n",
    "    target_q_values = target_model.predict(next_states, verbose=0)\n",
    "\n",
    "    # Update Q-values using Double DQN formula\n",
    "    targets = rewards + gamma * target_q_values[np.arange(batch_size), best_actions] * (1 - dones)\n",
    "\n",
    "    # Update main Q-network\n",
    "    q_values = model.predict(states, verbose=0)\n",
    "    q_values[np.arange(batch_size), actions] = targets\n",
    "    model.fit(states, q_values, epochs=1, verbose=0)\n",
    "\n",
    "    # Apply soft update to target network\n",
    "    if step % retrain_steps == 0:   \n",
    "        soft_update(model, target_model, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbU1uUo13Uu8",
    "outputId": "a17507bc-d5f1-4b2a-d0bc-a7ecd88a88a4"
   },
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "#model = build_model(state_size, action_size)\n",
    "#target_model = build_model(state_size, action_size)\n",
    "#target_model.set_weights(model.get_weights())  # Sync target network initially\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3rwB3iB3VQQ",
    "outputId": "cc1c7936-2319-4969-bd1e-08b04e1d18db",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_ddqn_with_params(params, num_episodes, verbose):\n",
    "    \n",
    "    global learning_rate, gamma, batch_size, tau, epsilon_decay, retrain_steps\n",
    "    learning_rate = params['learning_rate']\n",
    "    gamma = params['gamma']\n",
    "    batch_size = params['batch_size']\n",
    "    tau = params['tau']\n",
    "    epsilon_decay = params['epsilon_decay']\n",
    "    retrain_steps = params['retrain_steps']\n",
    "    \n",
    "    print('**************************************************************')\n",
    "    print('lr:', learning_rate, 'gamma', gamma, 'batch_size', batch_size, 'tau ', tau, 'epsilon decay', epsilon_decay,\n",
    "                 'retrain_steps ', retrain_steps)\n",
    "    \n",
    "    # Initialize models\n",
    "    model = build_model(state_size, action_size)\n",
    "    target_model = build_model(state_size, action_size)\n",
    "    target_model.set_weights(model.get_weights())  # Sync target network initially\n",
    "    \n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    episode_rewards = []\n",
    "    rolling_avg_rewards = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Epsilon-greedy policy\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(action_size)  # Explore\n",
    "            else:\n",
    "                action_vals = model.predict(state, verbose=0)\n",
    "                action = np.argmax(action_vals[0])  # Exploit\n",
    "\n",
    "            # Perform action\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store experience\n",
    "            store_experience(state, action, reward, next_state, done)\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "            # Train using experience replay\n",
    "            experience_replay_with_ddqn(model, target_model, batch_size, gamma, tau, step)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        # Record reward\n",
    "        episode_rewards.append(total_reward)\n",
    "        rolling_avg = np.mean(episode_rewards[-20:])\n",
    "        rolling_avg_rewards.append(rolling_avg)\n",
    "\n",
    "        # Print progress\n",
    "        if verbose:\n",
    "           print(f\"Episode: {episode+1:3}/{max_episodes}, Reward: {total_reward:+7.2f}, \"\n",
    "                f\"Epsilon: {epsilon:.2f}, Rolling Avg: {rolling_avg:4.2f}, Steps: {step:3}\")\n",
    "\n",
    "        # Check if environment is solved\n",
    "        if rolling_avg >= solved_threshold:\n",
    "            print(f\"Environment solved in {episode+1} episodes!\")\n",
    "            model.save(\"XXX_ddqn_model1.keras\")\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Rewards Rolling Avg: {rolling_avg:4.2f}\")\n",
    "    print(f\"Training completed in {(end_time - start_time)/60:.2f} minutes\")\n",
    "    print(learning_rate, gamma, batch_size, tau, epsilon_decay, retrain_steps)\n",
    "    \n",
    "    return (rolling_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.995 retrain_steps  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734794748.877377    1307 service.cc:146] XLA service 0x7f6a30005370 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1734794748.877401    1307 service.cc:154]   StreamExecutor device (0): NVIDIA T600 Laptop GPU, Compute Capability 7.5\n",
      "I0000 00:00:1734794750.925746    1307 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards Rolling Avg: 17.50\n",
      "Training completed in 2.00 minutes\n",
      "0.0001 0.99 128 0.15 0.995 30\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.995 retrain_steps  30\n",
      "Rewards Rolling Avg: 30.35\n",
      "Training completed in 3.45 minutes\n",
      "0.0001 0.99 128 0.1 0.995 30\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.995 retrain_steps  30\n",
      "Rewards Rolling Avg: 19.75\n",
      "Training completed in 3.04 minutes\n",
      "0.0001 0.99 128 0.001 0.995 30\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.995 retrain_steps  15\n",
      "Rewards Rolling Avg: 18.00\n",
      "Training completed in 2.46 minutes\n",
      "0.0001 0.99 128 0.15 0.995 15\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.995 retrain_steps  15\n",
      "Rewards Rolling Avg: 19.70\n",
      "Training completed in 2.72 minutes\n",
      "0.0001 0.99 128 0.1 0.995 15\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.995 retrain_steps  15\n",
      "Rewards Rolling Avg: 26.30\n",
      "Training completed in 3.24 minutes\n",
      "0.0001 0.99 128 0.001 0.995 15\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.995 retrain_steps  5\n",
      "Rewards Rolling Avg: 25.85\n",
      "Training completed in 3.39 minutes\n",
      "0.0001 0.99 128 0.15 0.995 5\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.995 retrain_steps  5\n",
      "Rewards Rolling Avg: 18.20\n",
      "Training completed in 2.72 minutes\n",
      "0.0001 0.99 128 0.1 0.995 5\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.995 retrain_steps  5\n",
      "Rewards Rolling Avg: 20.60\n",
      "Training completed in 2.91 minutes\n",
      "0.0001 0.99 128 0.001 0.995 5\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.995 retrain_steps  30\n",
      "Rewards Rolling Avg: 32.30\n",
      "Training completed in 3.80 minutes\n",
      "0.001 0.99 128 0.15 0.995 30\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.995 retrain_steps  30\n",
      "Rewards Rolling Avg: 41.15\n",
      "Training completed in 4.10 minutes\n",
      "0.001 0.99 128 0.1 0.995 30\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.995 retrain_steps  30\n",
      "Rewards Rolling Avg: 23.30\n",
      "Training completed in 2.89 minutes\n",
      "0.001 0.99 128 0.001 0.995 30\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.995 retrain_steps  15\n",
      "Rewards Rolling Avg: 25.00\n",
      "Training completed in 3.49 minutes\n",
      "0.001 0.99 128 0.15 0.995 15\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.995 retrain_steps  15\n",
      "Rewards Rolling Avg: 38.45\n",
      "Training completed in 3.80 minutes\n",
      "0.001 0.99 128 0.1 0.995 15\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.995 retrain_steps  15\n",
      "Rewards Rolling Avg: 17.35\n",
      "Training completed in 2.62 minutes\n",
      "0.001 0.99 128 0.001 0.995 15\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.995 retrain_steps  5\n",
      "Rewards Rolling Avg: 25.35\n",
      "Training completed in 3.48 minutes\n",
      "0.001 0.99 128 0.15 0.995 5\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.995 retrain_steps  5\n",
      "Rewards Rolling Avg: 43.25\n",
      "Training completed in 4.18 minutes\n",
      "0.001 0.99 128 0.1 0.995 5\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.995 retrain_steps  5\n",
      "Rewards Rolling Avg: 18.60\n",
      "Training completed in 2.50 minutes\n",
      "0.001 0.99 128 0.001 0.995 5\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.995 retrain_steps  30\n",
      "Rewards Rolling Avg: 27.75\n",
      "Training completed in 3.29 minutes\n",
      "0.01 0.99 128 0.15 0.995 30\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.995 retrain_steps  30\n",
      "Rewards Rolling Avg: 25.65\n",
      "Training completed in 3.41 minutes\n",
      "0.01 0.99 128 0.1 0.995 30\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.995 retrain_steps  30\n",
      "Rewards Rolling Avg: 22.70\n",
      "Training completed in 2.69 minutes\n",
      "0.01 0.99 128 0.001 0.995 30\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.995 retrain_steps  15\n",
      "Rewards Rolling Avg: 23.35\n",
      "Training completed in 3.17 minutes\n",
      "0.01 0.99 128 0.15 0.995 15\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.995 retrain_steps  15\n",
      "Rewards Rolling Avg: 25.10\n",
      "Training completed in 3.26 minutes\n",
      "0.01 0.99 128 0.1 0.995 15\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.995 retrain_steps  15\n",
      "Rewards Rolling Avg: 15.95\n",
      "Training completed in 2.42 minutes\n",
      "0.01 0.99 128 0.001 0.995 15\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.995 retrain_steps  5\n",
      "Rewards Rolling Avg: 27.00\n",
      "Training completed in 3.40 minutes\n",
      "0.01 0.99 128 0.15 0.995 5\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.995 retrain_steps  5\n",
      "Rewards Rolling Avg: 28.75\n",
      "Training completed in 3.52 minutes\n",
      "0.01 0.99 128 0.1 0.995 5\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.995 retrain_steps  5\n",
      "Rewards Rolling Avg: 22.35\n",
      "Training completed in 3.02 minutes\n",
      "0.01 0.99 128 0.001 0.995 5\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.99 retrain_steps  30\n",
      "Rewards Rolling Avg: 34.55\n",
      "Training completed in 3.78 minutes\n",
      "0.0001 0.99 128 0.15 0.99 30\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  30\n",
      "Rewards Rolling Avg: 12.00\n",
      "Training completed in 2.32 minutes\n",
      "0.0001 0.99 128 0.1 0.99 30\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.99 retrain_steps  30\n",
      "Rewards Rolling Avg: 47.70\n",
      "Training completed in 4.97 minutes\n",
      "0.0001 0.99 128 0.001 0.99 30\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.99 retrain_steps  15\n",
      "Rewards Rolling Avg: 25.55\n",
      "Training completed in 3.10 minutes\n",
      "0.0001 0.99 128 0.15 0.99 15\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  15\n",
      "Rewards Rolling Avg: 27.50\n",
      "Training completed in 3.22 minutes\n",
      "0.0001 0.99 128 0.1 0.99 15\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.99 retrain_steps  15\n",
      "Rewards Rolling Avg: 16.25\n",
      "Training completed in 2.53 minutes\n",
      "0.0001 0.99 128 0.001 0.99 15\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.99 retrain_steps  5\n",
      "Rewards Rolling Avg: 23.25\n",
      "Training completed in 3.00 minutes\n",
      "0.0001 0.99 128 0.15 0.99 5\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  5\n",
      "Rewards Rolling Avg: 12.90\n",
      "Training completed in 2.29 minutes\n",
      "0.0001 0.99 128 0.1 0.99 5\n",
      "**************************************************************\n",
      "lr: 0.0001 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.99 retrain_steps  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards Rolling Avg: 35.80\n",
      "Training completed in 4.02 minutes\n",
      "0.0001 0.99 128 0.001 0.99 5\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.99 retrain_steps  30\n",
      "Rewards Rolling Avg: 41.30\n",
      "Training completed in 5.18 minutes\n",
      "0.001 0.99 128 0.15 0.99 30\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  30\n",
      "Rewards Rolling Avg: 37.20\n",
      "Training completed in 4.15 minutes\n",
      "0.001 0.99 128 0.1 0.99 30\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.99 retrain_steps  30\n",
      "Rewards Rolling Avg: 21.85\n",
      "Training completed in 2.96 minutes\n",
      "0.001 0.99 128 0.001 0.99 30\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.99 retrain_steps  15\n",
      "Rewards Rolling Avg: 52.90\n",
      "Training completed in 5.19 minutes\n",
      "0.001 0.99 128 0.15 0.99 15\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  15\n",
      "Rewards Rolling Avg: 55.20\n",
      "Training completed in 5.53 minutes\n",
      "0.001 0.99 128 0.1 0.99 15\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.99 retrain_steps  15\n",
      "Rewards Rolling Avg: 26.10\n",
      "Training completed in 3.73 minutes\n",
      "0.001 0.99 128 0.001 0.99 15\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.99 retrain_steps  5\n",
      "Rewards Rolling Avg: 45.50\n",
      "Training completed in 4.84 minutes\n",
      "0.001 0.99 128 0.15 0.99 5\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  5\n",
      "Rewards Rolling Avg: 43.50\n",
      "Training completed in 5.16 minutes\n",
      "0.001 0.99 128 0.1 0.99 5\n",
      "**************************************************************\n",
      "lr: 0.001 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.99 retrain_steps  5\n",
      "Rewards Rolling Avg: 35.80\n",
      "Training completed in 4.24 minutes\n",
      "0.001 0.99 128 0.001 0.99 5\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.99 retrain_steps  30\n",
      "Rewards Rolling Avg: 29.45\n",
      "Training completed in 3.47 minutes\n",
      "0.01 0.99 128 0.15 0.99 30\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  30\n",
      "Rewards Rolling Avg: 24.45\n",
      "Training completed in 4.20 minutes\n",
      "0.01 0.99 128 0.1 0.99 30\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.99 retrain_steps  30\n",
      "Rewards Rolling Avg: 22.50\n",
      "Training completed in 3.50 minutes\n",
      "0.01 0.99 128 0.001 0.99 30\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.99 retrain_steps  15\n",
      "Rewards Rolling Avg: 25.25\n",
      "Training completed in 4.25 minutes\n",
      "0.01 0.99 128 0.15 0.99 15\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  15\n",
      "Rewards Rolling Avg: 32.20\n",
      "Training completed in 5.66 minutes\n",
      "0.01 0.99 128 0.1 0.99 15\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.99 retrain_steps  15\n",
      "Rewards Rolling Avg: 15.80\n",
      "Training completed in 3.52 minutes\n",
      "0.01 0.99 128 0.001 0.99 15\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.15 epsilon decay 0.99 retrain_steps  5\n",
      "Rewards Rolling Avg: 23.70\n",
      "Training completed in 4.70 minutes\n",
      "0.01 0.99 128 0.15 0.99 5\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  5\n",
      "Rewards Rolling Avg: 28.05\n",
      "Training completed in 5.40 minutes\n",
      "0.01 0.99 128 0.1 0.99 5\n",
      "**************************************************************\n",
      "lr: 0.01 gamma 0.99 batch_size 128 tau  0.001 epsilon decay 0.99 retrain_steps  5\n",
      "Rewards Rolling Avg: 17.15\n",
      "Training completed in 3.31 minutes\n",
      "0.01 0.99 128 0.001 0.99 5\n",
      "Top Hyperparameters:\n",
      "    batch_size  epsilon_decay  gamma  learning_rate  retrain_steps    tau  \\\n",
      "40         128           0.99   0.99         0.0010             15  0.100   \n",
      "39         128           0.99   0.99         0.0010             15  0.150   \n",
      "29         128           0.99   0.99         0.0001             30  0.001   \n",
      "42         128           0.99   0.99         0.0010              5  0.150   \n",
      "43         128           0.99   0.99         0.0010              5  0.100   \n",
      "\n",
      "    avg_reward  \n",
      "40        55.2  \n",
      "39        52.9  \n",
      "29        47.7  \n",
      "42        45.5  \n",
      "43        43.5  \n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "results = []\n",
    "for params in ParameterGrid(param_grid):\n",
    "    avg_reward = train_ddqn_with_params(params, 50, verbose)\n",
    "    results.append({**params, 'avg_reward': avg_reward})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='avg_reward', ascending=False)\n",
    "\n",
    "# Display top results\n",
    "print(\"Top Hyperparameters:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESKhagPNhmoE"
   },
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save(\"lunarlander_ddqn_model1.keras\")\n",
    "files.download(\"lunarlander_ddqn_model1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgSoBzkVhmq3"
   },
   "outputs": [],
   "source": [
    "# Plot rewards with rolling average\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episode_rewards, label='Rewards', color='blue')\n",
    "plt.plot(rolling_avg_rewards, label='Rolling Avg (Last 100 Episodes)', color='orange')\n",
    "plt.axhline(y=solved_threshold, color='red', linestyle='--', label='Solved Threshold')\n",
    "plt.title('Double DQN Training Performance')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Na_rWzOWriHF"
   },
   "outputs": [],
   "source": [
    "# Testing for 50 episodes\n",
    "start_time = time.time()\n",
    "\n",
    "for e_test in range(50):  # Run 50 test episodes\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    for t_test in range(max_steps):  # Use the same max_steps as training\n",
    "        # Use the trained model for testing\n",
    "        action_vals = model.predict(state, verbose=0)  # Predict action values\n",
    "        action = np.argmax(action_vals[0])  # Choose the action with the highest Q-value\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(f\"Test Episode: {e_test + 1}/50, Reward: {total_reward:.2f}\")\n",
    "            break\n",
    "\n",
    "end_time = time.time()\n",
    "testing_duration = (end_time - start_time) / 60  # Convert to minutes\n",
    "print(f\"Testing completed in {testing_duration:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfQ5pxsxhmtp"
   },
   "outputs": [],
   "source": [
    "# Test the trained agent with video rendering\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')  # Enable RGB rendering\n",
    "frames = []  # Store frames for visualization\n",
    "\n",
    "# Render a single test episode\n",
    "state, _ = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "tot_rewards = 0\n",
    "\n",
    "while True:\n",
    "    # Use the trained model for action\n",
    "    action_vals = model.predict(state, verbose=0)  # Predict action values\n",
    "    action = np.argmax(action_vals[0])  # Choose the action with the highest Q-value\n",
    "\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    frames.append(env.render())  # Save frame for rendering later\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    tot_rewards += reward\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        print(f\"Rendered Test Episode Reward: {tot_rewards:.2f}\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Save the rendered episode as a GIF\n",
    "def save_frames_as_gif(frames, path='./', filename='lunarlander_ddqn1.gif'):\n",
    "    images = [Image.fromarray(frame) for frame in frames]\n",
    "    gif_path = os.path.join(path, filename)\n",
    "    images[0].save(gif_path, save_all=True, append_images=images[1:], duration=50, loop=0)\n",
    "    print(f\"Saved GIF to: {gif_path}\")\n",
    "\n",
    "save_frames_as_gif(frames, filename=\"lunarlander_ddqn1.gif\")\n",
    "files.download(\"lunarlander_ddqn1.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YszwSDVhmwP"
   },
   "outputs": [],
   "source": [
    "Soft Updates Every Step: Use a small tau (e.g., 0.005).\n",
    "Soft Updates Every N Steps/Episodes: Allows a higher tau (e.g., 0.1).\n",
    "Hard Updates: Perform less frequently (e.g., every 1000 steps) target_model.set_weights(model.get_weights())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DL-Keras",
   "language": "python",
   "name": "dl-keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

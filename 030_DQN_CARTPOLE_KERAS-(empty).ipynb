{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb89376",
   "metadata": {},
   "source": [
    "### **CARTPOLE DQN KERAS**\n",
    "\n",
    "This is a naive version of the CARTPOLE DQN algorithm <br>\n",
    "Try to fill the missing parts\n",
    "\n",
    "This is the simplest approach of DQN with a single network (Remember the standard DQN uses 2 networks and trains one, in this one we use one and train one\n",
    "\n",
    "Fill the empty places and try to make it to work Ask for help if stocked!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "# Use mixed-precision training for faster computations on supported GPUs\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "import sys\n",
    "sys.stderr = open('err.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2041146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "devices = device_lib.list_local_devices()\n",
    "gpu_devices = [device for device in devices if device.device_type == 'GPU']\n",
    "for gpu in gpu_devices:\n",
    "    print('Using', gpu.physical_device_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2be125",
   "metadata": {},
   "source": [
    "#### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78010280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to fine tune\n",
    "# Try your own parameters\n",
    "# Remember epsilon and gamma are very important\n",
    "\n",
    "\n",
    "MAX_EPISODES = 300\n",
    "ROLLING_WINDOW = 20\n",
    "MEMORY_SIZE = 2000\n",
    "MAX_STEPS = 500\n",
    "\n",
    "gamma = 0.99                        # discount rate\n",
    "epsilon = 1.0                        # exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.99\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "solved_threshold = 195\n",
    "\n",
    "verb = 0                             # to see traces (verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa84c05",
   "metadata": {},
   "source": [
    "#### **Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# [CREATE YOUR NEURAL NETWORK TRY 16/32 or 24/24]\n",
    "###\n",
    "\n",
    "def build_model(state_size, action_size):\n",
    "    inputs = Input(shape=(state_size,), name=\"state_input\")\n",
    "    x = Dense(1, activation='relu', name=\"dense_1\")(inputs)\n",
    "    x = Dense(1, activation='relu', name=\"dense_2\")(x)\n",
    "    outputs = Dense(action_size, activation='linear', name=\"output_layer\")(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"Q_Network\")\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fca383",
   "metadata": {},
   "source": [
    "#### **Support Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761e2bb",
   "metadata": {},
   "source": [
    "$Q_{\\text{target}}(s_t, a_t) = r_t + \\gamma \\cdot \\max_{a'} Q(s_{t+1}, a') \\cdot (1 - \\text{done})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c07fd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "def store(state, action, reward, next_state, done):             \n",
    "    \"\"\"\n",
    "    This function appends the actual observation and reward to the Replay buffer\n",
    "    \"\"\"\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "def select_action_greedy(state, DQN):           \n",
    "    \"\"\"\n",
    "    Selects the if agent takes a random action (explore) or a predicted action (exploit)\n",
    "    \"\"\"\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    act_values = DQN.predict(state, verbose=verb)\n",
    "    return np.argmax(act_values[0])  # returns action selected with greedy strategy\n",
    "\n",
    "# Sample experiences from the replay buffer\n",
    "def sample_experiences(batch_size):\n",
    "    \"\"\"\n",
    "    Samples a batch_size of experiences from the Replay buffer. \n",
    "    You MUST transform the data into numpy arrays as this accelerates the response time sensibily\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(replay_buffer), batch_size, replace=False)\n",
    "    batch = [replay_buffer[i] for i in indices]\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    return (states, actions, rewards, next_states, dones)\n",
    "###\n",
    "# YOU MUST VECTIORIZE THE RETURN. TRANSFORM THE OUTPUTS IN np arrays otherwise it will be very slow\n",
    "###\n",
    "\n",
    "    \n",
    "def experience_replay(batch_size, model, epsilon):\n",
    "    \"\"\"\n",
    "    The critical function in the whole program\n",
    "    1. gets a minibatch from replay buffer\n",
    "    2. Predicts targets_qs from states (we need the full batch predicted but we'll avoid dones)\n",
    "    3. predicts next_qs from next_states\n",
    "    4. Bellman equation on next_qs obtains new target_qs $Q_{\\text{target}}(s_t, a_t) = r_t + \\gamma \\cdot \\max_{a'} Q(s_{t+1}, a') \\cdot (1 - \\text{done})$\n",
    "\n",
    "    6. Train DQN input states, output target_qs \n",
    "\n",
    "    \"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "    \n",
    "    # Predict Q-values for current and next states using vectorized operations\n",
    "    \n",
    "#### \n",
    "#  [ Predict target_qs with model predict states, predict next_qs with model predict next states]\n",
    "####\n",
    "    target_qs = \n",
    "    next_qs   = \n",
    "\n",
    "    # Update target Q-values using standard DQN logic   \n",
    "    target_qs[np.arange(batch_size), actions] = rewards + gamma * np.max(next_qs, axis=1) * (1 - dones)\n",
    "    \n",
    "    # Train the model on the Q-values\n",
    "    model.fit(states, target_qs, epochs=1, verbose=0)\n",
    "\n",
    "###\n",
    "#[Here you have a load and save weights. Can you make a breakpoint to restart the program after a while of training?]\n",
    "###\n",
    "\n",
    "def load(name, DQN):\n",
    "    DQN.load_weights(name)\n",
    "\n",
    "def save(name, DQN):\n",
    "    DQN.save_weights(name)\n",
    "\n",
    "DQN = build_model(state_size, action_size)\n",
    "DQN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb99f42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rewards_per_episode= []\n",
    "\n",
    "done = False\n",
    "rolling_avg = 0\n",
    "rolling_avg_rewards = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(MAX_EPISODES):                           # Should be While True, however we limit number of eps\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "            \n",
    "        action = select_action_greedy(state, DQN)\n",
    "        next_state, reward, done, truncated , _ = env.step(action)\n",
    "                    \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        store(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward = total_reward + reward\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            experience_replay(batch_size, DQN, epsilon)\n",
    "            \n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)            # decay epsilon\n",
    "    \n",
    "    rewards_per_episode.append(total_reward)\n",
    "    rolling_avg = np.mean(rewards_per_episode[-ROLLING_WINDOW:])   # append rewards\n",
    "    rolling_avg_rewards.append(rolling_avg)\n",
    "    print(f\"Episode: {e+1:3}/{MAX_EPISODES}, Reward: {total_reward:+7.2f}, \"\n",
    "          f\"Epsilon: {epsilon:.2f}, Rolling Avg: {rolling_avg:6.2f}, Steps: {step:3} Terminated: {done} \")\n",
    "\n",
    "            \n",
    "    # Check if environment is solved\n",
    "    if rolling_avg >= solved_threshold:\n",
    "        print(f\"Environment solved in {e+1} episodes!\")\n",
    "#            model.save(\"lunarlander_ddqn_model1.keras\")\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "testing_duration = (end_time - start_time) / 60  # Convert to minutes\n",
    "print(f\"Testing completed in {testing_duration:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5419d918",
   "metadata": {},
   "source": [
    "#### **Learning Plot and Episode Rewards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards with rolling average\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_per_episode, label='Rewards', color='blue')\n",
    "plt.plot(rolling_avg_rewards, label='Rolling Avg (Last 20 Episodes)', color='orange')\n",
    "plt.axhline(y=solved_threshold, color='red', linestyle='--', label='Solved Threshold')\n",
    "plt.title('DQN Training Performance CARTPOLE')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51baef8f",
   "metadata": {},
   "source": [
    "#### **Simulation - Testing 10 episodes with the DQN Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for 10 episodes\n",
    "start_time = time.time()\n",
    "\n",
    "for e_test in range(10):  # Run 10 test episodes\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    \n",
    "    steps = 0\n",
    "    while True:\n",
    "        # Use the trained model for testing\n",
    "        action_vals = DQN.predict(state, verbose=0)  # Predict action values\n",
    "        action = np.argmax(action_vals[0])  # Choose the action with the highest Q-value\n",
    "\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        steps = steps + 1\n",
    "        if done or truncated:\n",
    "            print(f\"Test Episode: {e_test + 1:2}/10, Reward: {total_reward:.2f}, Steps: {steps:3}\")\n",
    "            break\n",
    "\n",
    "end_time = time.time()\n",
    "testing_duration = (end_time - start_time) / 60  # Convert to minutes\n",
    "print(f\"Testing completed in {testing_duration:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f36b76",
   "metadata": {},
   "source": [
    "#### **Rendering 1 episode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e3acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent with video rendering\n",
    "# This code is useful if you are using colab otherwise use render_mode='human'\n",
    "env = gym.make((\"CartPole-v1\"), render_mode='rgb_array')  # Enable RGB rendering\n",
    "frames = []  # Store frames for visualization\n",
    "\n",
    "# Render a single test episode\n",
    "state, _ = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "tot_rewards = 0\n",
    "\n",
    "while True:\n",
    "    # Use the trained model for action\n",
    "    action_vals = DQN.predict(state, verbose=0)  # Predict action values\n",
    "    action = np.argmax(action_vals[0])           # Choose the action with the highest Q-value\n",
    "\n",
    "    next_state, reward, done, truncated, _ = env.step(action)\n",
    "    frames.append(env.render())                  # Save frame for rendering later\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    tot_rewards += reward\n",
    "    state = next_state\n",
    "\n",
    "    if done or truncated:\n",
    "        print(f\"Rendered Test Episode Reward: {tot_rewards:.2f}\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Save the rendered episode as a GIF\n",
    "def save_frames_as_gif(frames, path='./', filename='CARTPOLE_DQN.gif'):\n",
    "    images = [Image.fromarray(frame) for frame in frames]\n",
    "    gif_path = os.path.join(path, filename)\n",
    "    images[0].save(gif_path, save_all=True, append_images=images[1:], duration=50, loop=0)\n",
    "    print(f\"Saved GIF to: {gif_path}\")\n",
    "\n",
    "save_frames_as_gif(frames, filename='CARTPOLE_DQN.gif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba24e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "session_info.show(html=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Keras",
   "language": "python",
   "name": "dl-keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0a15ca",
   "metadata": {},
   "source": [
    "### Russell Value iteration and  Policy iteration\n",
    "#### Baseline program for the Practice 2\n",
    "Fill the right places in the program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e321e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import session_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b4e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99  # Discount factor\n",
    "theta = 1e-6  # Convergence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91b099aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "class RussellsGrid(gym.Env):\n",
    "    \"\"\"\n",
    "    RussellsGrid Env is a custom OpenAI Gym environment representing a 3x4 grid world with specific states, actions,\n",
    "    and rewards. The agent navigates through the grid aiming to reach the positive goal for a reward of +1,\n",
    "    avoiding the negative goal which leads to a reward of -1, and facing a slight penalty of -0.04 for each step taken.\n",
    "\n",
    "    Actions:\n",
    "    - 0: Up\n",
    "    - 1: Right\n",
    "    - 2: Down\n",
    "    - 3: Left\n",
    "\n",
    "    States:\n",
    "    - The agent's position, 0 to nrow*ncol-1\n",
    "\n",
    "    Rewards:\n",
    "    - +1 for reaching the positive goal.\n",
    "    - -1 for reaching the negative goal.\n",
    "    - -0.04 for each step taken.\n",
    "\n",
    "    Environment Dynamics:\n",
    "    - The agent moves deterministically in the intended direction (80% chance).\n",
    "    - There's a 20% chance of moving randomly to an adjacent cell.\n",
    "\n",
    "    Special Positions:\n",
    "    - Start position: (2, 0)\n",
    "    - Positive goal: (0, 3)\n",
    "    - Negative goal: (1, 3)\n",
    "    - Invalid position: (1, 1)\n",
    "\n",
    "    Rendering:\n",
    "    - Supports 'human' rendering mode to visually represent the grid with agent ('A'), positive goal ('G'),\n",
    "      negative goal ('R'), and invalid position ('X').\n",
    "\n",
    "    Usage:\n",
    "    - Instantiate the environment and use reset() to initialize the agent.\n",
    "    - Use step(action) to interact with the environment, returning the next state, reward, and done flag.\n",
    "    \"\"\"\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 4}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        self.ncol = 4\n",
    "        self.nrow = 3\n",
    "        self.size = (self.nrow, self.ncol)\n",
    "#        self.num_states = self.width * self.height\n",
    "\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(4)  # Up, Right, Down, Left\n",
    "        self.observation_space = spaces.Discrete(12)\n",
    "\n",
    "        # Define initial state\n",
    "        self.state = np.zeros(3)\n",
    "        self.start_position = np.array([2,0])\n",
    "        self.positive_goal = np.array([0, 3])\n",
    "        self.negative_goal = np.array([1, 3])\n",
    "        self.invalid_position = np.array([1, 1])\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Create transition matrix P\n",
    "        self.P = self._create_transition_matrix()\n",
    "\n",
    "    def _create_transition_matrix(self):\n",
    "        P = {s: {a: [] for a in range(4)} for s in range(self.observation_space.n)}\n",
    "\n",
    "        for s in range(self.observation_space.n):\n",
    "            row, col = divmod(s, self.ncol)\n",
    "            \n",
    "            if (row, col) == tuple(self.invalid_position):\n",
    "                continue  # Skip invalid position\n",
    "\n",
    "            for a in range(4):  # Up, Right, Down, Left\n",
    "                outcomes = self._get_action_outcomes(row, col, a)\n",
    "                \n",
    "                for next_state, prob in outcomes.items():\n",
    "                    next_row, next_col = divmod(next_state, self.ncol)\n",
    "                    \n",
    "                    # Determine reward and done status\n",
    "                    if (next_row, next_col) == tuple(self.positive_goal):\n",
    "                        reward = 1.0\n",
    "                        done = True\n",
    "                    elif (next_row, next_col) == tuple(self.negative_goal):\n",
    "                        reward = -1.0\n",
    "                        done = True\n",
    "                    else:\n",
    "                        reward = -0.04\n",
    "                        done = False\n",
    "\n",
    "                    P[s][a].append((prob, next_state, reward, done))\n",
    "\n",
    "        return P\n",
    "\n",
    "    def _get_action_outcomes(self, row, col, action):\n",
    "        outcomes = {}\n",
    "        intended_next_state = self._get_next_state(row, col, action)\n",
    "        outcomes[intended_next_state] = 0.8\n",
    "\n",
    "        # Add random adjacent cells with 0.2 probability\n",
    "        adjacent_actions = [0, 1, 2, 3]\n",
    "        adjacent_actions.remove(action)\n",
    "        for adj_action in adjacent_actions:\n",
    "            adj_next_state = self._get_next_state(row, col, adj_action)\n",
    "            if adj_next_state in outcomes:\n",
    "                outcomes[adj_next_state] += 0.2 / 3\n",
    "            else:\n",
    "                outcomes[adj_next_state] = 0.2 / 3\n",
    "\n",
    "        return outcomes\n",
    "\n",
    "    def _get_next_state(self, row, col, action):\n",
    "        next_row, next_col = row, col\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            next_row = max(0, row - 1)\n",
    "        elif action == 1:  # Right\n",
    "            next_col = min(self.ncol - 1, col + 1)\n",
    "        elif action == 2:  # Down\n",
    "            next_row = min(self.nrow - 1, row + 1)\n",
    "        elif action == 3:  # Left\n",
    "            next_col = max(0, col - 1)\n",
    "\n",
    "        # Check if next position is invalid\n",
    "        if (next_row, next_col) == tuple(self.invalid_position):\n",
    "            next_row, next_col = row, col  # Stay in current position\n",
    "\n",
    "        return next_row * self.ncol + next_col\n",
    "    \n",
    "    def cell_id(coord, width=4):\n",
    "        return coord[0]*width + coord[1]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset the agent to the starting position\n",
    "        self.state = self.cell_id(self.start_position)\n",
    "        \n",
    "        return self.state, {}\n",
    "    \n",
    "    def cell_id(self, coord, width=4):\n",
    "        return coord[0]*width + coord[1]\n",
    "\n",
    "    def step(self, action):\n",
    "        current_state = self.agent_position[0] * self.ncol + self.agent_position[1]\n",
    "        \n",
    "        # Randomly choose an outcome based on probabilities\n",
    "        outcomes = self.P[current_state][action]\n",
    "        probs = [outcome[0] for outcome in outcomes]\n",
    "        chosen_outcome = self.np_random.choice(len(outcomes), p=probs)\n",
    "        \n",
    "        _, next_state, reward, done = outcomes[chosen_outcome]\n",
    "        \n",
    "        # Update agent position\n",
    "        self.agent_position = np.array(divmod(next_state, self.ncol))\n",
    "\n",
    "        # Update the state\n",
    "        self.state = self._get_observation()\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self.render()\n",
    "\n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            for i in range(self.nrow):\n",
    "                for j in range(self.ncol):\n",
    "                    if np.array_equal([i, j], self.agent_position):\n",
    "                        print('A', end=' ')\n",
    "                    elif np.array_equal([i, j], self.positive_goal):\n",
    "                        print('G', end=' ')\n",
    "                    elif np.array_equal([i, j], self.negative_goal):\n",
    "                        print('R', end=' ')\n",
    "                    elif np.array_equal([i, j], self.invalid_position):\n",
    "                        print('X', end=' ')\n",
    "                    else:\n",
    "                        print('.', end=' ')\n",
    "                print()\n",
    "            print()\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4b96968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RussellsGrid(render_mode='human')\n",
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df1b44a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy state 0 and action 1\n",
      "[(0.8, 1, -0.04, False),\n",
      " (0.13333333333333333, 0, -0.04, False),\n",
      " (0.06666666666666667, 4, -0.04, False)]\n"
     ]
    }
   ],
   "source": [
    "# Visualizaing P\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"Policy state 0 and action 1\")\n",
    "pprint(env.P[0][1])\n",
    "\n",
    "# structure of P\n",
    "#Top level: state (an int representing the current state)\n",
    "\n",
    "# Second level: action (an int representing the action taken)\n",
    "\n",
    "# Third level: A list of possible transitions\n",
    "\n",
    "# Each transition: A tuple with\n",
    "#     probability: Probability of this transition (float)\n",
    "#     next_state: State the agent will move to (int)\n",
    "#     reward: Immediate reward received (float)\n",
    "#     done: Whether it's a terminal state (bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44528e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Loops\n",
    "\n",
    "def policy_evaluation(env, policy, theta=1e-8, gamma=1.0):\n",
    "    \"\"\" \n",
    "    Evaluates policy\n",
    "    \n",
    "    Arguments: env, policy, theta and Gamma\n",
    "    Returns:\n",
    "          V (value vector)\n",
    "    \n",
    "    \"\"\"\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            if s == env.invalid_position[0] * env.ncol + env.invalid_position[1]:\n",
    "                continue\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    v += action_prob * prob * (reward + gamma * V[next_state] * (not done))\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def policy_improvement(env, V, gamma=1.0):\n",
    "    \"\"\" \n",
    "    Computes greedy policy w.r.t a given env and value function.\n",
    "    \n",
    "    Arguments: env, value function, discount factor(gamma)\n",
    "    Returns:   policy\n",
    "    \"\"\"\n",
    "    policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    for s in range(env.observation_space.n):\n",
    "        if s == env.invalid_position[0] * env.ncol + env.invalid_position[1]:\n",
    "            continue\n",
    "        q_values = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                q_values[a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "        best_a = np.argmax(q_values)\n",
    "        policy[s] = np.eye(env.action_space.n)[best_a]\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(env, theta=1e-8, gamma=1.0):\n",
    "    policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\n",
    "    i=0\n",
    "    while True:\n",
    "        V = policy_evaluation(env, policy, theta, gamma)\n",
    "        new_policy = policy_improvement(env, V, gamma)\n",
    "        if np.all(np.abs(policy - new_policy) < theta):\n",
    "            print('Policy iterations', i)\n",
    "            break\n",
    "        policy = new_policy\n",
    "        i = i + 1\n",
    "    return policy, V\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b1215bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Loop\n",
    "def value_iteration(env, theta=1e-8, gamma=1.0):\n",
    "    \"\"\" \n",
    "    Value Iteration loop\n",
    "    \n",
    "    Arguments: env, theta and Gamma\n",
    "    Returns:\n",
    "         policy vector, V (value vector)\n",
    "    \"\"\"\n",
    "    \n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    i = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            if s == env.invalid_position[0] * env.ncol + env.invalid_position[1]:\n",
    "                continue\n",
    "            v = V[s]\n",
    "            q_values = np.zeros(env.action_space.n)\n",
    "            for a in range(env.action_space.n):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    q_values[a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "            V[s] = np.max(q_values)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            i = i+1\n",
    "        if delta < theta:\n",
    "            print('Value iterations', i)\n",
    "            break\n",
    "    \n",
    "    policy = policy_improvement(env, V, gamma)\n",
    "    return policy, V\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d45b970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supporting utilities\n",
    "def print_policy(policy, env):\n",
    "    policy_actions = np.argmax(policy, axis=1)\n",
    "    for i in range(env.nrow):\n",
    "        for j in range(env.ncol):\n",
    "            s = i * env.ncol + j\n",
    "            if np.array_equal([i, j], env.positive_goal):\n",
    "                print('G', end=' ')\n",
    "            elif np.array_equal([i, j], env.negative_goal):\n",
    "                print('N', end=' ')\n",
    "            elif np.array_equal([i, j], env.invalid_position):\n",
    "                print('X', end=' ')\n",
    "            else:\n",
    "                action = policy_actions[s]\n",
    "                if action == 0:\n",
    "                    print('↑', end=' ')\n",
    "                elif action == 1:\n",
    "                    print('→', end=' ')\n",
    "                elif action == 2:\n",
    "                    print('↓', end=' ')\n",
    "                elif action == 3:\n",
    "                    print('←', end=' ')\n",
    "        print()\n",
    "\n",
    "def print_value_function(V, env):\n",
    "    for i in range(env.nrow):\n",
    "        for j in range(env.ncol):\n",
    "            s = i * env.ncol + j\n",
    "            if np.array_equal([i, j], env.invalid_position):\n",
    "                print('  X  ', end=' ')\n",
    "            else:\n",
    "                print(f'{V[s]:5.2f}', end=' ')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be1d2a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration:\n",
      "Policy iterations 2\n",
      "Optimal Policy:\n",
      "→ → → G \n",
      "↑ X ↑ N \n",
      "↑ ← ↑ ← \n",
      "\n",
      "Optimal Value Function:\n",
      " 0.83  0.90  0.96  0.86 \n",
      " 0.77   X    0.75  0.81 \n",
      " 0.70  0.65  0.68  0.50 \n",
      "\n",
      "Value Iteration:\n",
      "Value iterations 220\n",
      "Optimal Policy:\n",
      "→ → → G \n",
      "↑ X ↑ N \n",
      "↑ ← ↑ ← \n",
      "\n",
      "Optimal Value Function:\n",
      " 0.83  0.90  0.96  0.86 \n",
      " 0.77   X    0.75  0.81 \n",
      " 0.70  0.65  0.68  0.50 \n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "env = RussellsGrid(render_mode='human')\n",
    "observation, info = env.reset()\n",
    "\n",
    "print(\"Policy Iteration:\")\n",
    "\n",
    "pi_policy, pi_V = policy_iteration(env, theta, gamma)\n",
    "\n",
    "print(\"Optimal Policy:\")\n",
    "print_policy(pi_policy, env)\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "print_value_function(pi_V, env)\n",
    "\n",
    "print(\"\\nValue Iteration:\")\n",
    "\n",
    "vi_policy, vi_V = value_iteration(env, theta, gamma)\n",
    "\n",
    "print(\"Optimal Policy:\")\n",
    "print_policy(vi_policy, env)\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "print_value_function(vi_V, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78784bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "gymnasium           1.0.0\n",
      "numpy               1.26.4\n",
      "session_info        1.0.0\n",
      "-----\n",
      "IPython             8.26.0\n",
      "jupyter_client      8.6.2\n",
      "jupyter_core        5.7.2\n",
      "-----\n",
      "Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]\n",
      "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "-----\n",
      "Session information updated at 2025-03-25 18:03\n"
     ]
    }
   ],
   "source": [
    "session_info.show(html=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc23df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

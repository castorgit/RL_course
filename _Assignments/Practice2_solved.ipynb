{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc54442",
   "metadata": {},
   "source": [
    "#### **Russell Value iteration and  Policy iteration**\n",
    "#### **Practice 2 Unsolved Version**\n",
    "Code the activities in the area marked as [TO CODE Activity x]\n",
    "See RL-SAMBD-Assignment2.pdf documentatgion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e321e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import session_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b4e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99  # Discount factor\n",
    "theta = 1e-6  # Convergence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b099aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RussellsGrid Environment\n",
    "class RussellsGrid(gym.Env):\n",
    "    \"\"\"\n",
    "    RussellsGrid Env is a custom OpenAI Gym environment representing a 3x4 grid world with specific states, actions,\n",
    "    and rewards. The agent navigates through the grid aiming to reach the positive goal for a reward of +1,\n",
    "    avoiding the negative goal which leads to a reward of -1, and facing a slight penalty of -0.04 for each step taken.\n",
    "\n",
    "    Actions:\n",
    "    - 0: Up\n",
    "    - 1: Right\n",
    "    - 2: Down\n",
    "    - 3: Left\n",
    "\n",
    "    States:\n",
    "    - The agent's position, 0 to nrow*ncol-1\n",
    "\n",
    "    Rewards:\n",
    "    - +1 for reaching the positive goal.\n",
    "    - -1 for reaching the negative goal.\n",
    "    - -0.04 for each step taken.\n",
    "\n",
    "    Environment Dynamics:\n",
    "    - The agent moves deterministically in the intended direction (80% chance).\n",
    "    - There's a 20% chance of moving randomly to an adjacent cell.\n",
    "\n",
    "    Special Positions:\n",
    "    - Start position: (2, 0)\n",
    "    - Positive goal: (0, 3)\n",
    "    - Negative goal: (1, 3)\n",
    "    - Invalid position: (1, 1)\n",
    "\n",
    "    Rendering:\n",
    "    - Supports 'human' rendering mode to visually represent the grid with agent ('A'), positive goal ('G'),\n",
    "      negative goal ('R'), and invalid position ('X').\n",
    "\n",
    "    Usage:\n",
    "    - Instantiate the environment and use reset() to initialize the agent.\n",
    "    - Use step(action) to interact with the environment, returning the next state, reward, and done flag.\n",
    "    \"\"\"\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 4}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        self.ncol = 4\n",
    "        self.nrow = 3\n",
    "        self.size = (self.nrow, self.ncol)\n",
    "#        self.num_states = self.width * self.height\n",
    "\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(4)  # Up, Right, Down, Left\n",
    "        self.observation_space = spaces.Discrete(12)\n",
    "\n",
    "        # Define initial state\n",
    "        self.state = np.zeros(3)\n",
    "        self.start_position = np.array([2,0])\n",
    "        self.positive_goal = np.array([0, 3])\n",
    "        self.negative_goal = np.array([1, 3])\n",
    "        self.invalid_position = np.array([1, 1])\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Create transition matrix P\n",
    "        self.P = self._create_transition_matrix()\n",
    "\n",
    "    def _create_transition_matrix(self):\n",
    "        P = {s: {a: [] for a in range(4)} for s in range(self.observation_space.n)}\n",
    "\n",
    "        for s in range(self.observation_space.n):\n",
    "            row, col = divmod(s, self.ncol)\n",
    "            \n",
    "            if (row, col) == tuple(self.invalid_position):\n",
    "                continue  # Skip invalid position\n",
    "\n",
    "            for a in range(4):  # Up, Right, Down, Left\n",
    "                outcomes = self._get_action_outcomes(row, col, a)\n",
    "                \n",
    "                for next_state, prob in outcomes.items():\n",
    "                    next_row, next_col = divmod(next_state, self.ncol)\n",
    "                    \n",
    "                    # Determine reward and done status\n",
    "                    if (next_row, next_col) == tuple(self.positive_goal):\n",
    "                        reward = 1.0\n",
    "                        done = True\n",
    "                    elif (next_row, next_col) == tuple(self.negative_goal):\n",
    "                        reward = -1.0\n",
    "                        done = True\n",
    "                    else:\n",
    "                        reward = -0.04\n",
    "                        done = False\n",
    "\n",
    "                    P[s][a].append((prob, next_state, reward, done))\n",
    "\n",
    "        return P\n",
    "\n",
    "    def _get_action_outcomes(self, row, col, action):\n",
    "        outcomes = {}\n",
    "        intended_next_state = self._get_next_state(row, col, action)\n",
    "        outcomes[intended_next_state] = 0.8\n",
    "\n",
    "        # Add random adjacent cells with 0.2 probability\n",
    "        adjacent_actions = [0, 1, 2, 3]\n",
    "        adjacent_actions.remove(action)\n",
    "        for adj_action in adjacent_actions:\n",
    "            adj_next_state = self._get_next_state(row, col, adj_action)\n",
    "            if adj_next_state in outcomes:\n",
    "                outcomes[adj_next_state] += 0.2 / 3\n",
    "            else:\n",
    "                outcomes[adj_next_state] = 0.2 / 3\n",
    "\n",
    "        return outcomes\n",
    "\n",
    "    def _get_next_state(self, row, col, action):\n",
    "        next_row, next_col = row, col\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            next_row = max(0, row - 1)\n",
    "        elif action == 1:  # Right\n",
    "            next_col = min(self.ncol - 1, col + 1)\n",
    "        elif action == 2:  # Down\n",
    "            next_row = min(self.nrow - 1, row + 1)\n",
    "        elif action == 3:  # Left\n",
    "            next_col = max(0, col - 1)\n",
    "\n",
    "        # Check if next position is invalid\n",
    "        if (next_row, next_col) == tuple(self.invalid_position):\n",
    "            next_row, next_col = row, col  # Stay in current position\n",
    "\n",
    "        return next_row * self.ncol + next_col\n",
    "    \n",
    "    def cell_id(coord, width=4):\n",
    "        return coord[0]*width + coord[1]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset the agent to the starting position\n",
    "        self.state = self.cell_id(self.start_position)\n",
    "        \n",
    "        return self.state, {}\n",
    "    \n",
    "    def cell_id(self, coord, width=4):\n",
    "        return coord[0]*width + coord[1]\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Convert agent position to state number\"\"\"\n",
    "        return self.agent_position[0] * self.ncol + self.agent_position[1]\n",
    "\n",
    "    def step(self, action):\n",
    "        current_state = self.agent_position[0] * self.ncol + self.agent_position[1]\n",
    "        \n",
    "        # Randomly choose an outcome based on probabilities\n",
    "        outcomes = self.P[current_state][action]\n",
    "        probs = [outcome[0] for outcome in outcomes]\n",
    "        chosen_outcome = self.np_random.choice(len(outcomes), p=probs)\n",
    "        \n",
    "        _, next_state, reward, done = outcomes[chosen_outcome]\n",
    "        \n",
    "        # Update agent position\n",
    "        self.agent_position = np.array(divmod(next_state, self.ncol))\n",
    "\n",
    "        # Update the state\n",
    "        self.state = self._get_observation()\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self.render()\n",
    "\n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            for i in range(self.nrow):\n",
    "                for j in range(self.ncol):\n",
    "                    if np.array_equal([i, j], self.agent_position):\n",
    "                        print('A', end=' ')\n",
    "                    elif np.array_equal([i, j], self.positive_goal):\n",
    "                        print('G', end=' ')\n",
    "                    elif np.array_equal([i, j], self.negative_goal):\n",
    "                        print('R', end=' ')\n",
    "                    elif np.array_equal([i, j], self.invalid_position):\n",
    "                        print('X', end=' ')\n",
    "                    else:\n",
    "                        print('.', end=' ')\n",
    "                print()\n",
    "            print()\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4936db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supporting functions\n",
    "def print_policy(env, policy):\n",
    "    \"\"\"Prints the policy in a grid format.\"\"\"\n",
    "    print(\"\\nValue Optimal Policy Format (3x4):\")\n",
    "    print(\"-\" * 50)\n",
    "    actions = ['↑', '→', '↓', '←']\n",
    "    \n",
    "    for i in range(env.nrow):\n",
    "        for j in range(env.ncol):\n",
    "            s = i * env.ncol + j\n",
    "            if (i, j) == tuple(env.invalid_position):\n",
    "                print('X', end=' ')\n",
    "            else:\n",
    "                a = np.argmax(policy[s])\n",
    "                print(actions[a], end=' ')\n",
    "        print()\n",
    "        \n",
    "    \n",
    "def print_value_function(V, env):\n",
    "    print(\"\\nValue Function Format (3x4):\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in range(env.nrow):\n",
    "        for j in range(env.ncol):\n",
    "\n",
    "            s = i * env.ncol + j\n",
    "            if np.array_equal([i, j], env.invalid_position):\n",
    "                print('  X  ', end=' ')\n",
    "            else:\n",
    "                print(f'{V[s]:5.3f}', end=' ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44528e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation Activity (Algorithm 1)\n",
    "# Policy Improvement (Algorithm 2)\n",
    "\n",
    "def policy_evaluation(env, policy, theta=1e-8, gamma=1.0):\n",
    "    \"\"\"Evaluates a policy by computing the value function.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment\n",
    "        policy: Array of shape [n_states, n_actions] representing the policy\n",
    "        theta: Convergence criterion\n",
    "        gamma: Discount factor\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    invalid_state = env.invalid_position[0] * env.ncol + env.invalid_position[1]\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            if s == invalid_state:\n",
    "                continue\n",
    "            \n",
    "            old_v = V[s]\n",
    "            new_v = 0\n",
    "            \n",
    "            # Calculate new value based on policy\n",
    "            for a, action_prob in enumerate(policy[s]):                           # for each s in S do\n",
    "                for prob, next_state, reward, done in env.P[s][a]:                 \n",
    "                    # If done, don't include future rewards\n",
    "                    future_value = 0 if done else gamma * V[next_state]           # Line 6 in Algorithm\n",
    "                    new_v += action_prob * prob * (reward + future_value)         # Line 6 in Algorithm\n",
    "            \n",
    "            V[s] = new_v\n",
    "            delta = max(delta, np.abs(old_v - new_v))                             # Line 7 in Algorithm\n",
    "            \n",
    "        if delta < theta:                                                         # until Delta < theta\n",
    "            break\n",
    "            \n",
    "    return V\n",
    "\n",
    "def policy_improvement(env, V, gamma=1.0):\n",
    "    \"\"\"Computes an improved policy based on a value function.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment\n",
    "        V: Array of shape [n_states] representing the value function\n",
    "        gamma: Discount factor\n",
    "    \"\"\"\n",
    "    policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    invalid_state = env.invalid_position[0] * env.ncol + env.invalid_position[1]\n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        if s == invalid_state:\n",
    "            continue\n",
    "            \n",
    "        # Compute Q-values for all actions\n",
    "        q_values = np.zeros(env.action_space.n)                                # Initialization\n",
    "        for a in range(env.action_space.n):                                    # for each state in environment\n",
    "            for prob, next_state, reward, done in env.P[s][a]:                 # Compute Q values for all actions\n",
    "                future_value = 0 if done else gamma * V[next_state]\n",
    "                q_values[a] += prob * (reward + future_value)                  # Lines 9 and 10 Policy improvement\n",
    "        \n",
    "        # Choose the action(s) with highest Q-value\n",
    "        best_actions = np.where(q_values == np.max(q_values))[0]               # Line 16 \n",
    "        # Split probability equally among best actions (in case of ties)\n",
    "        policy[s, best_actions] = 1.0 / len(best_actions)\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65cdda11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, theta=1e-8, gamma=1.0, max_iterations=1000):\n",
    "    \"\"\"Computes optimal policy using policy iteration.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment\n",
    "        theta: Convergence criterion\n",
    "        gamma: Discount factor\n",
    "        max_iterations: Maximum number of iterations\n",
    "    \"\"\"\n",
    "    # Initialize uniform random policy\n",
    "    policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\n",
    "    iterations = 0\n",
    "    \n",
    "    while iterations < max_iterations:\n",
    "        # Policy Evaluation\n",
    "        V = policy_evaluation(env, policy, theta, gamma)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        new_policy = policy_improvement(env, V, gamma)\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.all(np.abs(policy - new_policy) < theta):\n",
    "            break\n",
    "            \n",
    "        policy = new_policy\n",
    "        iterations += 1\n",
    "    \n",
    "    print(f'Policy iteration converged after {iterations} iterations')\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a852ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration LOOP Activity 2\n",
    "def value_iteration(env, theta=1e-8, gamma=1.0, max_iterations=1000):\n",
    "    \"\"\"Computes optimal policy using value iteration.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment\n",
    "        theta: Convergence criterion\n",
    "        gamma: Discount factor\n",
    "        max_iterations: Maximum number of iterations\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    invalid_state = env.invalid_position[0] * env.ncol + env.invalid_position[1]\n",
    "    iterations = 0\n",
    "    \n",
    "    while iterations < max_iterations:\n",
    "        delta = 0\n",
    "        \n",
    "        for s in range(env.observation_space.n):                           # for all s in Environment\n",
    "            if s == invalid_state:\n",
    "                continue\n",
    "                \n",
    "            old_v = V[s]\n",
    "            \n",
    "            # Compute Q-values for all actions\n",
    "            q_values = np.zeros(env.action_space.n)                       # initialize Q table\n",
    "            for a in range(env.action_space.n):                           # For all states\n",
    "                for prob, next_state, reward, done in env.P[s][a]:        # prob * Reward * gamma V' - Line 6 Algorithm 3\n",
    "                    future_value = 0 if done else gamma * V[next_state]\n",
    "                    q_values[a] += prob * (reward + future_value)\n",
    "            \n",
    "            V[s] = np.max(q_values)\n",
    "            delta = max(delta, np.abs(old_v - V[s]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "        iterations += 1\n",
    "    \n",
    "    print(f'Value iteration converged after {iterations} iterations')\n",
    "    policy = policy_improvement(env, V, gamma)\n",
    "    return policy, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2392fd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration converged after 3 iterations\n",
      "\n",
      "Policy from Policy Iteration:\n",
      "\n",
      "Value Optimal Policy Format (3x4):\n",
      "--------------------------------------------------\n",
      "→ → → ↑ \n",
      "↑ X ↑ ↑ \n",
      "↑ → ↑ ← \n",
      "Value iteration converged after 20 iterations\n",
      "\n",
      "Policy from Value Iteration:\n",
      "\n",
      "Value Optimal Policy Format (3x4):\n",
      "--------------------------------------------------\n",
      "→ → → ↑ \n",
      "↑ X ↑ ↑ \n",
      "↑ → ↑ ← \n",
      "\n",
      "Value Function Format (3x4):\n",
      "--------------------------------------------------\n",
      "0.607 0.758 0.931 0.853 \n",
      "0.477   X   0.634 0.783 \n",
      "0.370 0.376 0.485 0.278 \n"
     ]
    }
   ],
   "source": [
    "env = RussellsGrid()\n",
    "\n",
    "# Run policy iteration\n",
    "pi_policy, pi_value = policy_iteration(env, theta=1e-8, gamma=0.9)\n",
    "print(\"\\nPolicy from Policy Iteration:\")\n",
    "print_policy(env, pi_policy)\n",
    "\n",
    "\n",
    "# Run value iteration\n",
    "vi_policy, vi_value = value_iteration(env, theta=1e-8, gamma=0.9)\n",
    "print(\"\\nPolicy from Value Iteration:\")\n",
    "print_policy(env, vi_policy)\n",
    "print_value_function(vi_value, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78784bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "gymnasium           1.0.0\n",
      "numpy               1.26.4\n",
      "session_info        1.0.0\n",
      "-----\n",
      "IPython             8.26.0\n",
      "jupyter_client      8.6.2\n",
      "jupyter_core        5.7.2\n",
      "-----\n",
      "Python 3.12.3 (main, Sep 11 2024, 14:17:37) [GCC 13.2.0]\n",
      "Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "-----\n",
      "Session information updated at 2024-11-02 18:42\n"
     ]
    }
   ],
   "source": [
    "session_info.show(html=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "766d382f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.8666666666666667, 0, -0.04, False),\n",
       "   (0.06666666666666667, 1, -0.04, False),\n",
       "   (0.06666666666666667, 4, -0.04, False)],\n",
       "  1: [(0.8, 1, -0.04, False),\n",
       "   (0.13333333333333333, 0, -0.04, False),\n",
       "   (0.06666666666666667, 4, -0.04, False)],\n",
       "  2: [(0.8, 4, -0.04, False),\n",
       "   (0.13333333333333333, 0, -0.04, False),\n",
       "   (0.06666666666666667, 1, -0.04, False)],\n",
       "  3: [(0.8666666666666667, 0, -0.04, False),\n",
       "   (0.06666666666666667, 1, -0.04, False),\n",
       "   (0.06666666666666667, 4, -0.04, False)]},\n",
       " 1: {0: [(0.8666666666666667, 1, -0.04, False),\n",
       "   (0.06666666666666667, 2, -0.04, False),\n",
       "   (0.06666666666666667, 0, -0.04, False)],\n",
       "  1: [(0.8, 2, -0.04, False),\n",
       "   (0.13333333333333333, 1, -0.04, False),\n",
       "   (0.06666666666666667, 0, -0.04, False)],\n",
       "  2: [(0.8666666666666667, 1, -0.04, False),\n",
       "   (0.06666666666666667, 2, -0.04, False),\n",
       "   (0.06666666666666667, 0, -0.04, False)],\n",
       "  3: [(0.8, 0, -0.04, False),\n",
       "   (0.13333333333333333, 1, -0.04, False),\n",
       "   (0.06666666666666667, 2, -0.04, False)]},\n",
       " 2: {0: [(0.8, 2, -0.04, False),\n",
       "   (0.06666666666666667, 3, 1.0, True),\n",
       "   (0.06666666666666667, 6, -0.04, False),\n",
       "   (0.06666666666666667, 1, -0.04, False)],\n",
       "  1: [(0.8, 3, 1.0, True),\n",
       "   (0.06666666666666667, 2, -0.04, False),\n",
       "   (0.06666666666666667, 6, -0.04, False),\n",
       "   (0.06666666666666667, 1, -0.04, False)],\n",
       "  2: [(0.8, 6, -0.04, False),\n",
       "   (0.06666666666666667, 2, -0.04, False),\n",
       "   (0.06666666666666667, 3, 1.0, True),\n",
       "   (0.06666666666666667, 1, -0.04, False)],\n",
       "  3: [(0.8, 1, -0.04, False),\n",
       "   (0.06666666666666667, 2, -0.04, False),\n",
       "   (0.06666666666666667, 3, 1.0, True),\n",
       "   (0.06666666666666667, 6, -0.04, False)]},\n",
       " 3: {0: [(0.8666666666666667, 3, 1.0, True),\n",
       "   (0.06666666666666667, 7, -1.0, True),\n",
       "   (0.06666666666666667, 2, -0.04, False)],\n",
       "  1: [(0.8666666666666667, 3, 1.0, True),\n",
       "   (0.06666666666666667, 7, -1.0, True),\n",
       "   (0.06666666666666667, 2, -0.04, False)],\n",
       "  2: [(0.8, 7, -1.0, True),\n",
       "   (0.13333333333333333, 3, 1.0, True),\n",
       "   (0.06666666666666667, 2, -0.04, False)],\n",
       "  3: [(0.8, 2, -0.04, False),\n",
       "   (0.13333333333333333, 3, 1.0, True),\n",
       "   (0.06666666666666667, 7, -1.0, True)]},\n",
       " 4: {0: [(0.8, 0, -0.04, False),\n",
       "   (0.13333333333333333, 4, -0.04, False),\n",
       "   (0.06666666666666667, 8, -0.04, False)],\n",
       "  1: [(0.8666666666666667, 4, -0.04, False),\n",
       "   (0.06666666666666667, 0, -0.04, False),\n",
       "   (0.06666666666666667, 8, -0.04, False)],\n",
       "  2: [(0.8, 8, -0.04, False),\n",
       "   (0.06666666666666667, 0, -0.04, False),\n",
       "   (0.13333333333333333, 4, -0.04, False)],\n",
       "  3: [(0.8666666666666667, 4, -0.04, False),\n",
       "   (0.06666666666666667, 0, -0.04, False),\n",
       "   (0.06666666666666667, 8, -0.04, False)]},\n",
       " 5: {0: [], 1: [], 2: [], 3: []},\n",
       " 6: {0: [(0.8, 2, -0.04, False),\n",
       "   (0.06666666666666667, 7, -1.0, True),\n",
       "   (0.06666666666666667, 10, -0.04, False),\n",
       "   (0.06666666666666667, 6, -0.04, False)],\n",
       "  1: [(0.8, 7, -1.0, True),\n",
       "   (0.06666666666666667, 2, -0.04, False),\n",
       "   (0.06666666666666667, 10, -0.04, False),\n",
       "   (0.06666666666666667, 6, -0.04, False)],\n",
       "  2: [(0.8, 10, -0.04, False),\n",
       "   (0.06666666666666667, 2, -0.04, False),\n",
       "   (0.06666666666666667, 7, -1.0, True),\n",
       "   (0.06666666666666667, 6, -0.04, False)],\n",
       "  3: [(0.8, 6, -0.04, False),\n",
       "   (0.06666666666666667, 2, -0.04, False),\n",
       "   (0.06666666666666667, 7, -1.0, True),\n",
       "   (0.06666666666666667, 10, -0.04, False)]},\n",
       " 7: {0: [(0.8, 3, 1.0, True),\n",
       "   (0.06666666666666667, 7, -1.0, True),\n",
       "   (0.06666666666666667, 11, -0.04, False),\n",
       "   (0.06666666666666667, 6, -0.04, False)],\n",
       "  1: [(0.8, 7, -1.0, True),\n",
       "   (0.06666666666666667, 3, 1.0, True),\n",
       "   (0.06666666666666667, 11, -0.04, False),\n",
       "   (0.06666666666666667, 6, -0.04, False)],\n",
       "  2: [(0.8, 11, -0.04, False),\n",
       "   (0.06666666666666667, 3, 1.0, True),\n",
       "   (0.06666666666666667, 7, -1.0, True),\n",
       "   (0.06666666666666667, 6, -0.04, False)],\n",
       "  3: [(0.8, 6, -0.04, False),\n",
       "   (0.06666666666666667, 3, 1.0, True),\n",
       "   (0.06666666666666667, 7, -1.0, True),\n",
       "   (0.06666666666666667, 11, -0.04, False)]},\n",
       " 8: {0: [(0.8, 4, -0.04, False),\n",
       "   (0.06666666666666667, 9, -0.04, False),\n",
       "   (0.13333333333333333, 8, -0.04, False)],\n",
       "  1: [(0.8, 9, -0.04, False),\n",
       "   (0.06666666666666667, 4, -0.04, False),\n",
       "   (0.13333333333333333, 8, -0.04, False)],\n",
       "  2: [(0.8666666666666667, 8, -0.04, False),\n",
       "   (0.06666666666666667, 4, -0.04, False),\n",
       "   (0.06666666666666667, 9, -0.04, False)],\n",
       "  3: [(0.8666666666666667, 8, -0.04, False),\n",
       "   (0.06666666666666667, 4, -0.04, False),\n",
       "   (0.06666666666666667, 9, -0.04, False)]},\n",
       " 9: {0: [(0.8666666666666667, 9, -0.04, False),\n",
       "   (0.06666666666666667, 10, -0.04, False),\n",
       "   (0.06666666666666667, 8, -0.04, False)],\n",
       "  1: [(0.8, 10, -0.04, False),\n",
       "   (0.13333333333333333, 9, -0.04, False),\n",
       "   (0.06666666666666667, 8, -0.04, False)],\n",
       "  2: [(0.8666666666666667, 9, -0.04, False),\n",
       "   (0.06666666666666667, 10, -0.04, False),\n",
       "   (0.06666666666666667, 8, -0.04, False)],\n",
       "  3: [(0.8, 8, -0.04, False),\n",
       "   (0.13333333333333333, 9, -0.04, False),\n",
       "   (0.06666666666666667, 10, -0.04, False)]},\n",
       " 10: {0: [(0.8, 6, -0.04, False),\n",
       "   (0.06666666666666667, 11, -0.04, False),\n",
       "   (0.06666666666666667, 10, -0.04, False),\n",
       "   (0.06666666666666667, 9, -0.04, False)],\n",
       "  1: [(0.8, 11, -0.04, False),\n",
       "   (0.06666666666666667, 6, -0.04, False),\n",
       "   (0.06666666666666667, 10, -0.04, False),\n",
       "   (0.06666666666666667, 9, -0.04, False)],\n",
       "  2: [(0.8, 10, -0.04, False),\n",
       "   (0.06666666666666667, 6, -0.04, False),\n",
       "   (0.06666666666666667, 11, -0.04, False),\n",
       "   (0.06666666666666667, 9, -0.04, False)],\n",
       "  3: [(0.8, 9, -0.04, False),\n",
       "   (0.06666666666666667, 6, -0.04, False),\n",
       "   (0.06666666666666667, 11, -0.04, False),\n",
       "   (0.06666666666666667, 10, -0.04, False)]},\n",
       " 11: {0: [(0.8, 7, -1.0, True),\n",
       "   (0.13333333333333333, 11, -0.04, False),\n",
       "   (0.06666666666666667, 10, -0.04, False)],\n",
       "  1: [(0.8666666666666667, 11, -0.04, False),\n",
       "   (0.06666666666666667, 7, -1.0, True),\n",
       "   (0.06666666666666667, 10, -0.04, False)],\n",
       "  2: [(0.8666666666666667, 11, -0.04, False),\n",
       "   (0.06666666666666667, 7, -1.0, True),\n",
       "   (0.06666666666666667, 10, -0.04, False)],\n",
       "  3: [(0.8, 10, -0.04, False),\n",
       "   (0.06666666666666667, 7, -1.0, True),\n",
       "   (0.13333333333333333, 11, -0.04, False)]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = RussellsGrid()\n",
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdefc18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

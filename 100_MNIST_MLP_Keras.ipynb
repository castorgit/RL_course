{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebddc4d",
   "metadata": {},
   "source": [
    "#### **MNIST MLP with Keras**\n",
    "\n",
    "This is a template of an MNIST classifier with an Artificial Neural Network (MLP)\n",
    "If follows the basic blocks of a Deep Learning classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366ae7c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import ReLU, Dense, Softmax\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9298d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "\n",
    "num_classes = 10  # Digits 0-9\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72ef22",
   "metadata": {},
   "source": [
    "#### **Reading MNIST Dataset**\n",
    "Most frameworks offer an easy way to download the MNIST dataset. In this case we use the datasets method\n",
    "\n",
    "This is the way that Keras reads the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3556bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daed1920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)\n",
    "# The loaded data are numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d7a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# Preprocess the data (flattening and normalization)\n",
    "x_train = x_train.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef3cfa",
   "metadata": {},
   "source": [
    "#### **ANN architecture definition**\n",
    "This is the key part of the notebook, in this cell we define the structure of the network \n",
    "and all the different elements mainly\n",
    "\n",
    "- Activation function \n",
    "- Layer size\n",
    "- Number of layers\n",
    "\n",
    "In this example you have 3 models. Give them a try. To use one model or the other just make sure the model() construct is based on the parameters you want to try \n",
    "\n",
    "(you can do this by deleting the line \"model()=\" in the models you don't want to use)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "080e2e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Model 1 hidden layer input size 784, output layer 10 (num of classes)\n",
    "# ANN sizing details\n",
    "hidden_size = 256\n",
    "\n",
    "inputs = Input(shape= (784,))\n",
    "l1 = Dense(hidden_size)(inputs)\n",
    "l1 = ReLU()(l1)\n",
    "l1 = Dense(10)(l1)\n",
    "output = Softmax()(l1)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa6a89ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8475 - loss: 0.5532 - val_accuracy: 0.9470 - val_loss: 0.1857\n",
      "Epoch 2/5\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9549 - loss: 0.1553 - val_accuracy: 0.9635 - val_loss: 0.1260\n",
      "Epoch 3/5\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9740 - loss: 0.0967 - val_accuracy: 0.9677 - val_loss: 0.1083\n",
      "Epoch 4/5\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9781 - loss: 0.0725 - val_accuracy: 0.9688 - val_loss: 0.1024\n",
      "Epoch 5/5\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9845 - loss: 0.0533 - val_accuracy: 0.9728 - val_loss: 0.0924\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe1ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac340e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', color='orange')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f296d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "session_info.show(html=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69f4e0",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Keras",
   "language": "python",
   "name": "dl-keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

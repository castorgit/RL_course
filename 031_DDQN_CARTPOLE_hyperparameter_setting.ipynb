{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DDQN with Hyperparameter setting**\n",
    "\n",
    "This is an example of Hyperparameter setting for CARTPOLE. <br>\n",
    "With the actual setup it can take many hours (even with a GPU) <br>\n",
    "Press the colab icon to run it in COLAB (in this way you save your laptop for other tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFJSVpVdhkJ6",
    "outputId": "1d20e9ec-d059-44ea-ddad-df428c96ba22"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/castorgit/RL_course/blob/main/00_LunarLander-COLAB_render.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 09:03:38.984694: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-22 09:03:39.005671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-22 09:03:39.028796: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-22 09:03:39.035193: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-22 09:03:39.052362: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-22 09:03:40.214392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.0\n",
      "2.17.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as k\n",
    "print(k.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "ERROR: unknown command \"pandas\"\n"
     ]
    }
   ],
   "source": [
    "#### **Install Packages**\n",
    "!pip install gymnasium[box2d]\n",
    "!pip install numpy\n",
    "!pip install random\n",
    "!pip install matplotlib\n",
    "!pip install tensorflow==2.17.1\n",
    "!pip install keras==3.6.0\n",
    "!pip pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XmduteTzhmVH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734854625.895886  898937 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1734854625.927105  898937 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1734854625.927192  898937 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import time\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rxswekI1hmXw"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]  # 8 state variables\n",
    "action_size = env.action_space.n  # 4 discrete actions\n",
    "tf.random.set_seed(221)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **hyperparameter search space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.0001],\n",
    "    'gamma': [0.99],\n",
    "    'batch_size': [128],\n",
    "    'tau': [0.1],\n",
    "    'epsilon_decay': [0.99],\n",
    "    'retrain_steps': [10],\n",
    "    'initial_lr' : [0.001, 0.01, 0.1],\n",
    "    'd_steps'    : [10000, 50000, 5000],\n",
    "    'd_rate'     : [0.99, 0.98, 0.97]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zonyAYX-hmad"
   },
   "outputs": [],
   "source": [
    "#learning_rate = 0.0001\n",
    "gamma = 0.98\n",
    "batch_size = 128\n",
    "epsilon_start = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.99\n",
    "tau = 0.15                         # For soft target network update\n",
    "buffer_capacity = 10000\n",
    "max_episodes = 100                   # We train only 50 to see how the agent learns\n",
    "max_steps = 1200\n",
    "solved_threshold = 200\n",
    "verbose = 0                        # 0: No trace 1: Trace\n",
    "\n",
    "d_steps = 10000\n",
    "d_rate  = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IVrsk55q1qJY"
   },
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "replay_buffer = deque(maxlen=buffer_capacity)\n",
    "\n",
    "# Add experience to replay buffer\n",
    "def store_experience(state, action, reward, next_state, done):\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "# Sample experiences from the replay buffer\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.choice(len(replay_buffer), batch_size, replace=False)\n",
    "    batch = [replay_buffer[i] for i in indices]\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    return (\n",
    "        np.vstack(states),\n",
    "        np.array(actions),\n",
    "        np.array(rewards),\n",
    "        np.vstack(next_states),\n",
    "        np.array(dones, dtype=np.float32)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Neural Network definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network model\n",
    "def build_model(state_size, action_size):\n",
    "    inputs = Input(shape=(state_size,))  \n",
    "    x = Dense(16, activation=\"relu\")(inputs)\n",
    "#    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "#    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    outputs = Dense(action_size, activation=\"linear\")(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    \n",
    "    lr_schedule = ExponentialDecay(\n",
    "                            initial_learning_rate=initial_lr,\n",
    "                            decay_steps = d_steps,\n",
    "                            decay_rate  = d_rate,\n",
    "                            staircase=True)                                  \n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=\"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Support Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bcqSPBvyw3OQ"
   },
   "outputs": [],
   "source": [
    "# Soft update function for the target network\n",
    "def soft_update(model, target_model, tau):\n",
    "    target_weights = target_model.get_weights()\n",
    "    model_weights = model.get_weights()\n",
    "    new_weights = [\n",
    "        tau * mw + (1 - tau) * tw for mw, tw in zip(model_weights, target_weights)\n",
    "    ]\n",
    "    target_model.set_weights(new_weights)\n",
    "\n",
    "# Polski Optimization Function\n",
    "def polski_optimization(weights, beta=0.01):\n",
    "    return [w * (1 - beta) for w in weights]\n",
    "\n",
    "# Double DQN target calculation\n",
    "def experience_replay_with_ddqn(model, target_model, batch_size, gamma, tau, step):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "\n",
    "    # Predict Q-values for next states using both networks\n",
    "    next_q_values = model.predict(next_states, verbose=0)\n",
    "    best_actions = np.argmax(next_q_values, axis=1)\n",
    "    target_q_values = target_model.predict(next_states, verbose=0)\n",
    "\n",
    "    # Update Q-values using Double DQN formula\n",
    "    targets = rewards + gamma * target_q_values[np.arange(batch_size), best_actions] * (1 - dones)\n",
    "\n",
    "    # Update main Q-network\n",
    "    q_values = model.predict(states, verbose=0)\n",
    "    q_values[np.arange(batch_size), actions] = targets\n",
    "    model.fit(states, q_values, epochs=1, verbose=0)\n",
    "\n",
    "    # Apply soft update to target network\n",
    "    if step % retrain_steps == 0:   \n",
    "        soft_update(model, target_model, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbU1uUo13Uu8",
    "outputId": "a17507bc-d5f1-4b2a-d0bc-a7ecd88a88a4"
   },
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "#model = build_model(state_size, action_size)\n",
    "#target_model = build_model(state_size, action_size)\n",
    "#target_model.set_weights(model.get_weights())  # Sync target network initially\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3rwB3iB3VQQ",
    "outputId": "cc1c7936-2319-4969-bd1e-08b04e1d18db",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_ddqn_with_params(params, max_episodes, verbose):\n",
    "    \n",
    "    global initial_lr, gamma, batch_size, tau, epsilon_decay, retrain_steps\n",
    "    initial_lr = params['initial_lr']\n",
    "    gamma = params['gamma']\n",
    "    batch_size = params['batch_size']\n",
    "    tau = params['tau']\n",
    "    epsilon_decay = params['epsilon_decay']\n",
    "    retrain_steps = params['retrain_steps']\n",
    "    d_steps = params['d_steps']\n",
    "    d_rate = params['d_rate']\n",
    "    \n",
    "    print('**************************************************************')\n",
    "    print('ini_lr:', initial_lr, 'gamma', gamma, 'batch_size', batch_size, 'tau ', tau, 'epsilon decay', epsilon_decay,\n",
    "                 'retrain_steps ', retrain_steps, 'd_steps', d_steps, 'd_rate', d_rate)\n",
    "    \n",
    "    # Initialize models\n",
    "    model = build_model(state_size, action_size)\n",
    "    target_model = build_model(state_size, action_size)\n",
    "    target_model.set_weights(model.get_weights())  # Sync target network initially\n",
    "    \n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    episode_rewards = []\n",
    "    rolling_avg_rewards = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Epsilon-greedy policy\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(action_size)  # Explore\n",
    "            else:\n",
    "                action_vals = model.predict(state, verbose=0)\n",
    "                action = np.argmax(action_vals[0])  # Exploit\n",
    "\n",
    "            # Perform action\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store experience\n",
    "            store_experience(state, action, reward, next_state, done)\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "            # Train using experience replay\n",
    "            experience_replay_with_ddqn(model, target_model, batch_size, gamma, tau, step)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        # Record reward\n",
    "        episode_rewards.append(total_reward)\n",
    "        rolling_avg = np.mean(episode_rewards[-20:])\n",
    "        rolling_avg_rewards.append(rolling_avg)\n",
    "\n",
    "        # Print progress\n",
    "        if verbose:\n",
    "           print(f\"Episode: {episode+1:3}/{max_episodes}, Reward: {total_reward:+7.2f}, \"\n",
    "                f\"Epsilon: {epsilon:.2f}, Rolling Avg: {rolling_avg:4.2f}, Steps: {step:3}\")\n",
    "\n",
    "        # Check if environment is solved\n",
    "        if rolling_avg >= solved_threshold:\n",
    "            print(f\"Environment solved in {episode+1} episodes!\")\n",
    "            model.save(\"XXX_ddqn_model1.keras\")\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Rewards Rolling Avg: {rolling_avg:4.2f}\")\n",
    "    print(f\"Training completed in {(end_time - start_time)/60:.2f} minutes\")\n",
    "    print(initial_lr, d_steps, d_rate, gamma, batch_size, tau, epsilon_decay, retrain_steps)\n",
    "    \n",
    "    return (rolling_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************\n",
      "ini_lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 10000 d_rate 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734854626.046520  898937 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1734854626.046752  898937 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1734854626.046803  898937 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1734854627.817360  898937 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1734854627.817519  898937 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-22 09:03:47.817538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1734854627.817600  898937 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-22 09:03:47.817629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1144 MB memory:  -> device: 0, name: NVIDIA T600 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734854629.298001  905330 service.cc:146] XLA service 0x7fa200005740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1734854629.298046  905330 service.cc:154]   StreamExecutor device (0): NVIDIA T600 Laptop GPU, Compute Capability 7.5\n",
      "2024-12-22 09:03:49.305339: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-22 09:03:50.819982: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "I0000 00:00:1734854630.928684  905330 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards Rolling Avg: 81.65\n",
      "Training completed in 19.94 minutes\n",
      "0.001 10000 0.99 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 10000 d_rate 0.99\n",
      "Rewards Rolling Avg: 35.35\n",
      "Training completed in 15.17 minutes\n",
      "0.01 10000 0.99 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.1 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 10000 d_rate 0.99\n",
      "Rewards Rolling Avg: 19.50\n",
      "Training completed in 9.53 minutes\n",
      "0.1 10000 0.99 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 50000 d_rate 0.99\n",
      "Rewards Rolling Avg: 57.70\n",
      "Training completed in 24.40 minutes\n",
      "0.001 50000 0.99 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 50000 d_rate 0.99\n",
      "Rewards Rolling Avg: 42.35\n",
      "Training completed in 12.44 minutes\n",
      "0.01 50000 0.99 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.1 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 50000 d_rate 0.99\n",
      "Rewards Rolling Avg: 31.80\n",
      "Training completed in 9.38 minutes\n",
      "0.1 50000 0.99 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 5000 d_rate 0.99\n",
      "Rewards Rolling Avg: 79.25\n",
      "Training completed in 21.85 minutes\n",
      "0.001 5000 0.99 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 5000 d_rate 0.99\n",
      "Rewards Rolling Avg: 49.00\n",
      "Training completed in 15.39 minutes\n",
      "0.01 5000 0.99 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.1 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 5000 d_rate 0.99\n",
      "Rewards Rolling Avg: 19.50\n",
      "Training completed in 10.55 minutes\n",
      "0.1 5000 0.99 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 10000 d_rate 0.98\n",
      "Rewards Rolling Avg: 66.45\n",
      "Training completed in 27.73 minutes\n",
      "0.001 10000 0.98 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 10000 d_rate 0.98\n",
      "Rewards Rolling Avg: 34.70\n",
      "Training completed in 10.48 minutes\n",
      "0.01 10000 0.98 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.1 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 10000 d_rate 0.98\n",
      "Rewards Rolling Avg: 20.25\n",
      "Training completed in 6.50 minutes\n",
      "0.1 10000 0.98 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 50000 d_rate 0.98\n",
      "Rewards Rolling Avg: 57.10\n",
      "Training completed in 14.28 minutes\n",
      "0.001 50000 0.98 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 50000 d_rate 0.98\n",
      "Rewards Rolling Avg: 39.45\n",
      "Training completed in 9.64 minutes\n",
      "0.01 50000 0.98 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.1 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 50000 d_rate 0.98\n",
      "Rewards Rolling Avg: 16.25\n",
      "Training completed in 6.52 minutes\n",
      "0.1 50000 0.98 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 5000 d_rate 0.98\n",
      "Rewards Rolling Avg: 43.30\n",
      "Training completed in 14.09 minutes\n",
      "0.001 5000 0.98 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 5000 d_rate 0.98\n",
      "Rewards Rolling Avg: 33.90\n",
      "Training completed in 8.50 minutes\n",
      "0.01 5000 0.98 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.1 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 5000 d_rate 0.98\n",
      "Rewards Rolling Avg: 14.40\n",
      "Training completed in 5.69 minutes\n",
      "0.1 5000 0.98 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 10000 d_rate 0.97\n",
      "Rewards Rolling Avg: 45.40\n",
      "Training completed in 13.06 minutes\n",
      "0.001 10000 0.97 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 10000 d_rate 0.97\n",
      "Rewards Rolling Avg: 40.40\n",
      "Training completed in 9.28 minutes\n",
      "0.01 10000 0.97 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.1 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 10000 d_rate 0.97\n",
      "Rewards Rolling Avg: 16.20\n",
      "Training completed in 6.25 minutes\n",
      "0.1 10000 0.97 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 50000 d_rate 0.97\n",
      "Rewards Rolling Avg: 72.85\n",
      "Training completed in 15.62 minutes\n",
      "0.001 50000 0.97 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 50000 d_rate 0.97\n",
      "Rewards Rolling Avg: 39.25\n",
      "Training completed in 9.17 minutes\n",
      "0.01 50000 0.97 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.1 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 50000 d_rate 0.97\n",
      "Rewards Rolling Avg: 18.15\n",
      "Training completed in 6.57 minutes\n",
      "0.1 50000 0.97 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.001 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 5000 d_rate 0.97\n",
      "Rewards Rolling Avg: 72.15\n",
      "Training completed in 14.59 minutes\n",
      "0.001 5000 0.97 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.01 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 5000 d_rate 0.97\n",
      "Rewards Rolling Avg: 40.25\n",
      "Training completed in 9.68 minutes\n",
      "0.01 5000 0.97 0.99 128 0.1 0.99 10\n",
      "**************************************************************\n",
      "ini_lr: 0.1 gamma 0.99 batch_size 128 tau  0.1 epsilon decay 0.99 retrain_steps  10 d_steps 5000 d_rate 0.97\n",
      "Rewards Rolling Avg: 32.45\n",
      "Training completed in 7.88 minutes\n",
      "0.1 5000 0.97 0.99 128 0.1 0.99 10\n",
      "Top Hyperparameters:\n",
      "    batch_size  d_rate  d_steps  epsilon_decay  gamma  initial_lr  \\\n",
      "0          128    0.99    10000           0.99   0.99       0.001   \n",
      "6          128    0.99     5000           0.99   0.99       0.001   \n",
      "21         128    0.97    50000           0.99   0.99       0.001   \n",
      "24         128    0.97     5000           0.99   0.99       0.001   \n",
      "9          128    0.98    10000           0.99   0.99       0.001   \n",
      "\n",
      "    learning_rate  retrain_steps  tau  avg_reward  \n",
      "0          0.0001             10  0.1       81.65  \n",
      "6          0.0001             10  0.1       79.25  \n",
      "21         0.0001             10  0.1       72.85  \n",
      "24         0.0001             10  0.1       72.15  \n",
      "9          0.0001             10  0.1       66.45  \n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "results = []\n",
    "for params in ParameterGrid(param_grid):\n",
    "    avg_reward = train_ddqn_with_params(params, max_episodes, verbose)\n",
    "    results.append({**params, 'avg_reward': avg_reward})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='avg_reward', ascending=False)\n",
    "\n",
    "# Display top results\n",
    "print(\"Top Hyperparameters:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "PIL                 11.0.0\n",
      "gymnasium           1.0.0\n",
      "keras               3.6.0\n",
      "matplotlib          3.9.2\n",
      "numpy               1.26.4\n",
      "pandas              2.2.3\n",
      "session_info        1.0.0\n",
      "sklearn             1.5.2\n",
      "tensorflow          2.17.1\n",
      "-----\n",
      "IPython             8.28.0\n",
      "jupyter_client      8.6.3\n",
      "jupyter_core        5.7.2\n",
      "-----\n",
      "Python 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]\n",
      "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "-----\n",
      "Session information updated at 2024-12-22 14:38\n"
     ]
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show(html=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YszwSDVhmwP"
   },
   "source": [
    "#### **Some rules of thumb**\n",
    "\n",
    "Soft Updates Every Step: Use a small tau (e.g., 0.005).<br>\n",
    "Soft Updates Every N Steps/Episodes: Allows a higher tau (e.g., 0.1).<br>\n",
    "Hard Updates: Perform less frequently (e.g., every 1000 steps) target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DL-Keras",
   "language": "python",
   "name": "dl-keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
